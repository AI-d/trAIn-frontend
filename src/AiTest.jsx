import React, { useEffect, useRef, useState } from 'react';
import axios from "axios";
import {playAudio} from "openai/helpers/audio";

const AiTest = () => {
    const pcRef = useRef(null);
    const wsRef = useRef(null);      // ì‹œê·¸ë„ë§ WS
    const audioWs = useRef(null);    // ì˜¤ë””ì˜¤ WS
    const localStreamRef = useRef(null); // localStream ì €ì¥ìš©
    const [connected, setConnected] = useState(false);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const [isAiResponding, setIsAiResponding] = useState(false);


    const isSpeakingRef = useRef(false);

    const initConnection = async () => {
        const scenarioId = 1;
        const userId = 1;


        try {
            console.log("ì„¸ì…˜ ìƒì„± ìš”ì²­ ì¤‘...");
            const response = await axios.post("http://localhost:9090/api/sessions", {
                userId: userId,
                scenarioId: scenarioId
            });

            const sessionId = response.data.data.sessionId;
            console.log("ì„¸ì…˜ ì•„ì´ë”” ìƒì„± ì™„ë£Œ: ", sessionId);

            // ---------------------------
            // 1. ì˜¤ë””ì˜¤ WS ë¨¼ì € ì—°ê²° (SESSION_INIT ì „ì†¡ìš©)
            // ---------------------------
            audioWs.current = new WebSocket(`ws://localhost:9090/ws/audio/${sessionId}`);
            audioWs.current.binaryType = "arraybuffer";

            audioWs.current.onopen = () => {
                console.log("ì˜¤ë””ì˜¤ WS ì—°ê²°ë¨");

                // SESSION_INIT ì „ì†¡
                const initMessage = {
                    type: "SESSION_INIT",
                    scenarioId: scenarioId
                };
                console.log("SESSION_INIT ì „ì†¡:", initMessage);
                audioWs.current.send(JSON.stringify(initMessage));
            };

            audioWs.current.onmessage = (event) => {
                if (event.data instanceof ArrayBuffer) {
                    console.log("GPT ìŒì„± ì‘ë‹µ ìˆ˜ì‹ :", event.data.byteLength, "bytes");
                    // TODO: ìŒì„± ì¬ìƒ
                    playAudio(event.data);
                }
            };

            audioWs.current.onerror = (error) => {
                console.error("ì˜¤ë””ì˜¤ WS ì—ëŸ¬:", error);
            };

            audioWs.current.onclose = () => {
                console.log("ì˜¤ë””ì˜¤ WS ì¢…ë£Œ");
            };

            // ---------------------------
            // 2. ì‹œê·¸ë„ë§ WebSocket ì—°ê²°
            // ---------------------------
            wsRef.current = new WebSocket(`ws://localhost:9090/ws/signaling/${sessionId}`);

            wsRef.current.onopen = async () => {
                console.log("ì‹œê·¸ë„ë§ WS ì—°ê²°ë¨");
                setConnected(true);

                // ---------------------------
                // 3. RTCPeerConnection ìƒì„±
                // ---------------------------
                pcRef.current = new RTCPeerConnection({
                    iceServers: [{ urls: "stun:stun.l.google.com:19302" }],
                });

                // ë‚´ ì˜¤ë””ì˜¤ ê°€ì ¸ì˜¤ê¸°
                console.log("ë§ˆì´í¬ ì ‘ê·¼ ìš”ì²­...");
                const localStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 16000
                    },
                    video: false
                });
                console.log("ë§ˆì´í¬ ì ‘ê·¼ ì„±ê³µ");

                localStreamRef.current = localStream;

                // Audio íŠ¸ë™ WebRTCì— ì¶”ê°€
                localStream.getTracks().forEach((track) => {
                    pcRef.current.addTrack(track, localStream);
                });

                // ICE Candidate ì„œë²„ ì „ì†¡
                pcRef.current.onicecandidate = (event) => {
                    if (event.candidate) {
                        console.log("ICE Candidate ì „ì†¡");
                        wsRef.current.send(JSON.stringify({
                            type: "ICE_CANDIDATE",
                            candidate: event.candidate
                        }));
                    }
                };

                // Offer ìƒì„± ë° ì „ì†¡
                const offer = await pcRef.current.createOffer();
                await pcRef.current.setLocalDescription(offer);
                console.log("Offer ì „ì†¡");
                wsRef.current.send(JSON.stringify({ type: "OFFER", sdp: offer.sdp }));

                // ---------------------------
                // 4. AudioContext & AudioWorklet ì„¤ì •
                // ---------------------------
                console.log("AudioWorklet ì„¤ì • ì‹œì‘...");
                const audioContext = new AudioContext({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(localStream);

                try {
                    await audioContext.audioWorklet.addModule('/audio-processor.js');
                    console.log("AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ");

                    const workletNode = new AudioWorkletNode(audioContext, 'audio-processor');

                    workletNode.port.onmessage = (event) => {

                        if(!isSpeakingRef.current) return;

                        // Float32 -> Int16 ë³€í™˜
                        const float32Array = new Float32Array(event.data);
                        const int16Array = new Int16Array(float32Array.length);

                        for (let i = 0; i < float32Array.length; i++) {
                            const clamped = Math.max(-1, Math.min(1, float32Array[i]));
                            int16Array[i] = clamped * 32767;
                        }

                        // GPTë¡œ ìŒì„± ì „ì†¡
                        if (audioWs.current && audioWs.current.readyState === WebSocket.OPEN) {
                            audioWs.current.send(int16Array.buffer);
                        }
                    };

                    source.connect(workletNode);
                    console.log("AudioWorklet ì—°ê²° ì™„ë£Œ");

                } catch (error) {
                    console.error("AudioWorklet ì„¤ì • ì‹¤íŒ¨:", error);
                }
            };

            // ---------------------------
            // 5. ì‹œê·¸ë„ë§ ë©”ì‹œì§€ ì²˜ë¦¬
            // ---------------------------
            wsRef.current.onmessage = async (event) => {
                const data = JSON.parse(event.data);
                console.log("ì‹œê·¸ë„ë§ ë©”ì‹œì§€:", data.type);

                if (data.type === "ANSWER") {
                    await pcRef.current.setRemoteDescription({
                        type: 'answer',
                        sdp: data.sdp,
                    });
                    console.log("Remote description ì„¤ì • ì™„ë£Œ");
                } else if (data.type === "ICE_CANDIDATE" && data.candidate) {
                    await pcRef.current.addIceCandidate(data.candidate);
                    console.log("ICE Candidate ì¶”ê°€");
                }
            };

            wsRef.current.onerror = (error) => {
                console.error("ì‹œê·¸ë„ë§ WS ì—ëŸ¬:", error);
            };

            // ---------------------------
            // 6. ì‹œê·¸ë„ë§ ì¢…ë£Œ ì²˜ë¦¬
            // ---------------------------
            wsRef.current.onclose = () => {
                console.log("ì‹œê·¸ë„ë§ WS ì¢…ë£Œ");
                setConnected(false);
                audioWs.current?.close();
                pcRef.current?.close();
            };

        } catch (error) {
            console.error("ì—°ê²° ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
            setConnected(false);
        }
    };

    const handleSpeakToggle = () => {
        if (!connected || isAiResponding) return;

        if (!isSpeaking) {
            // ë§í•˜ê¸° ì‹œì‘
            console.log("ğŸ™ ë§í•˜ê¸° ì‹œì‘");
            setIsSpeaking(true);
            isSpeakingRef.current = true;

        } else {
            // ë§í•˜ê¸° ì¢…ë£Œ
            console.log("ë§í•˜ê¸° ì¢…ë£Œ");
            setIsSpeaking(false);
            isSpeakingRef.current = false;

            if (audioWs.current && audioWs.current.readyState === WebSocket.OPEN) {
                audioWs.current.send(JSON.stringify({ type: "speech.end" }));
                console.log("speech.end ì „ì†¡ ì™„ë£Œ");
                setIsAiResponding(true);
            }
        }
    };

    const closeConnection = () => {
        console.log("ì—°ê²° ì¢…ë£Œ ì‹œì‘...");

        wsRef.current?.close();
        audioWs.current?.close();
        pcRef.current?.close();

        if (localStreamRef.current) {
            localStreamRef.current.getTracks().forEach(track => track.stop());
        }

        setConnected(false);
        setIsSpeaking(false);
        setIsAiResponding(false);

        console.log("ì—°ê²° í•´ì œ ì™„ë£Œ");
    };

    useEffect(() => {
        return () => {
            closeConnection();
        };
    }, []);

    return (
        <div style={{ padding: 20, fontFamily: "Arial, sans-serif" }}>
            <h2>ğŸ§ GPT Realtime ëŒ€í™” í…ŒìŠ¤íŠ¸</h2>

            {/* ì—°ê²° ë²„íŠ¼ */}
            <div style={{ display: "flex", gap: 10, marginBottom: 20 }}>
                <button
                    onClick={initConnection}
                    disabled={connected}
                    style={{
                        padding: "10px 20px",
                        backgroundColor: connected ? "#ccc" : "#4CAF50",
                        color: "white",
                        border: "none",
                        borderRadius: 4,
                    }}
                >
                    ğŸš€ ì—°ê²°
                </button>
                <button
                    onClick={closeConnection}
                    disabled={!connected}
                    style={{
                        padding: "10px 20px",
                        backgroundColor: !connected ? "#ccc" : "#f44336",
                        color: "white",
                        border: "none",
                        borderRadius: 4,
                    }}
                >
                    ğŸ›‘ í•´ì œ
                </button>
                <span>
          ìƒíƒœ:{" "}
                    {connected ? (
                        <b style={{ color: "green" }}>â— ì—°ê²°ë¨</b>
                    ) : (
                        <b style={{ color: "gray" }}>â—‹ ì—°ê²° ì•ˆë¨</b>
                    )}
        </span>
            </div>

            {/* íƒ­ìœ¼ë¡œ ë§í•˜ê¸° */}
            <div
                style={{
                    display: "flex",
                    flexDirection: "column",
                    gap: "15px",
                    alignItems: "center",
                    marginTop: 20,
                }}
            >
                <button
                    onClick={handleSpeakToggle}
                    disabled={!connected || isAiResponding}
                    style={{
                        width: 150,
                        height: 150,
                        borderRadius: "50%",
                        fontSize: 20,
                        border: "none",
                        color: "white",
                        backgroundColor: isSpeaking
                            ? "#f44336"
                            : isAiResponding
                                ? "#2196F3"
                                : "#4CAF50",
                        cursor: connected ? "pointer" : "not-allowed",
                    }}
                >
                    {isSpeaking
                        ? "ğŸ—£ ë§í•˜ëŠ” ì¤‘"
                        : isAiResponding
                            ? "ğŸ¤– ì‘ë‹µ ì¤‘"
                            : "ğŸ¤ ëˆŒëŸ¬ì„œ ë§í•˜ê¸°"}
                </button>
            </div>
        </div>
    );
};

export default AiTest;